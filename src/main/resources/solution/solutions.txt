04-DF-spark-DSL.ipynb

1 show all employees and their max salary over time
`employee_salaries_df = salaries_df\
    .groupBy(col("emp_no"))\
    .max("salary")\
    .select(col("max(salary)").alias("salary"), col("emp_no"))
employees_df.join(employee_salaries_df, "emp_no")\
    .select(\
       concat(col("first_name"), lit(" "), col("last_name")).alias("fill_name"),\
       col("salary")\
).show()`
________________________________________________________


2 show all employees who were never managers
`employees_df.join(dept_managers_df, "emp_no", "left_anti")
#.where("emp_no=110800")
.show()`
________________________________________________________
3 for every employee, find the difference between their salary (current/latest) and the max salary of their department (departments table) """
`dept_average_salary = salaries_df.join(dept_emp_df, "emp_no").groupBy("dept_no").max("salary").select(col("dept_no"), col("max(salary)").alias('max_dept_salary'))
employer_resent_salary = salaries_df.orderBy(col("from_date").desc()).dropDuplicates(['emp_no']).select(col("emp_no"), col("salary"))
employees_with_dept = employees_df.join(dept_emp_df, "emp_no", "left").select(col("emp_no"), col("dept_no"), concat(col("first_name"), lit(" "), col("last_name")).alias("full_name"));
employees_with_dept_and_salary = employees_with_dept.join(employer_resent_salary, "emp_no").distinct()

employees_with_dept_and_salary\
            .join(dept_average_salary, "dept_no", "left")\
            .withColumn('salary_diff_with_department', col("salary") - col("max_dept_salary"))\
            .select(col("full_name"),col("salary_diff_with_department"))\
#            .where("salary_diff_with_department = 0")\
            .show()`


_______________________________________________________________________
05-DF-spark-SQL.ipynb

1 show all employees and their max salary over time
spark.sql("""SELECT concat(emp.first_name, ' ', emp.last_name) as full_name, max(salaries.salary) as salary 
        FROM employees emp
            LEFT JOIN salaries ON emp.emp_no = salaries.emp_no
        GROUP BY emp.first_name, emp.last_name
""").show()
________________________________________________________

2 show all employees who were never managers
#dept_managers_df.show()

spark.sql("""SELECT concat(emp.first_name, ' ', emp.last_name) as full_name, emp_no
        FROM employees emp
            LEFT ANTI JOIN dept_manager ON emp.emp_no = dept_manager.emp_no
""").show()

# WHERE emp.emp_no=110420
________________________________________________________

06-DF-Types.ipynb

Filter the cars DF, return all cars whose name contains either element of the list:
- contains function
 regexes

def get_car_names():
    return ["Volkswagen", "Mercedes-Benz", "Ford"]

# v1 - regexes
regexString = "|".join(get_car_names()) # Volkswagen|Mercedes-Benz|Ford
cars_interest_df = cars_df.select(
        col("Name"),
        regexp_extract(lower(col("Name")), regexString, 0).alias("regex_extract")
    ).filter(col("regex_extract") != "").orderBy(col("regex_extract"))

cars_interest_df.show(5, False)
_______________________
# v2 - contains
from functools import reduce

car_name_filters = [col("Name").contains(car_name.lower()) for car_name in ["Volkswagen", "Mercedes-Benz", "Ford"]]
big_filter = reduce(lambda filter1, filter2: filter1 | filter2, car_name_filters)
filtered_cars = cars_df.filter(big_filter)

filtered_cars.show(5, False)
________________________________________________________
Write a structured query that “explodes” an array of structs (of open and close hours).

monday_df = business_df. \
        select(col("business_id"), col("full_address"), lit("Monday").alias("day"), col("hours.Monday.open").alias("open_time"), col("hours.Monday.close").alias("close_time"))
tuesday_df = business_df. \
        select(col("business_id"), col("full_address"), lit("Tuesday").alias("day"), col("hours.Tuesday.open").alias("open_time"), col("hours.Tuesday.close").alias("close_time"))
friday_df = business_df. \
        select(col("business_id"), col("full_address"), lit("Friday").alias("day"), col("hours.Friday.open").alias("open_time"), col("hours.Friday.close").alias("close_time"))
monday_df.union(tuesday_df)\
    .union(friday_df)\
    .filter(col("open_time").isNotNull() & col("close_time").isNotNull()).show()