{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bc0cda5-e4ea-4c08-a0fa-fde8ff1eb3f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o73.jdbc.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:39)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:33)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\n\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:294)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m DBPARAMS \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m: password,\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver\u001b[39m\u001b[38;5;124m\"\u001b[39m: driver\n\u001b[1;32m     25\u001b[0m }\n\u001b[1;32m     27\u001b[0m employees \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpublic.employees\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 29\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memployees\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDBPARAMS\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:529\u001b[0m, in \u001b[0;36mDataFrameReader.jdbc\u001b[0;34m(self, url, table, column, lowerBound, upperBound, numPartitions, predicates, properties)\u001b[0m\n\u001b[1;32m    527\u001b[0m     jpredicates \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mtoJArray(gateway, gateway\u001b[38;5;241m.\u001b[39mjvm\u001b[38;5;241m.\u001b[39mjava\u001b[38;5;241m.\u001b[39mlang\u001b[38;5;241m.\u001b[39mString, predicates)\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mjdbc(url, table, jpredicates, jprop))\n\u001b[0;32m--> 529\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjprop\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o73.jdbc.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:39)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:33)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\n\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:294)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, from_json, date_format, to_timestamp\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    appName(\"Data Sources\"). \\\n",
    "    master(\"local\"). \\\n",
    "    config(\"spark.jars\", \"jars/postgresql-42.2.19.jar\"). \\\n",
    "    getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 14)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890e9ec7-f46c-4aa0-b552-d12592a2b37e",
   "metadata": {},
   "source": [
    "# Read/Write DataFrame with file system, HDFS, S3, FTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8090ce6d-fc9c-40a8-9c7b-6f46ddaf8caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config(\"spark.python.worker.memory\", \"8g\"). \\\n",
    "#     config(\"spark.driver.memory\", \"8g\"). \\\n",
    "#     config(\"spark.executor.memory\", \"8g\"). \\\n",
    "#  \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c32a3e7-9ded-4b4d-bc9e-a5979ee0a9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+------------+----------+----------------+--------------------+------+-------------+----------+\n",
      "|Acceleration|Cylinders|Displacement|Horsepower|Miles_per_Gallon|                Name|Origin|Weight_in_lbs|      Year|\n",
      "+------------+---------+------------+----------+----------------+--------------------+------+-------------+----------+\n",
      "|        12.0|        8|       307.0|       130|            18.0|chevrolet chevell...|   USA|         3504|      null|\n",
      "|        12.0|        8|       307.0|       130|            18.0|chevrolet chevell...|   USA|         3504|1970-01-01|\n",
      "|        11.5|        8|       350.0|       165|            15.0|   buick skylark 320|   USA|         3693|1970-01-01|\n",
      "|        11.0|        8|       318.0|       150|            18.0|  plymouth satellite|   USA|         3436|1970-01-01|\n",
      "|        12.0|        8|       304.0|       150|            16.0|       amc rebel sst|   USA|         3433|1970-01-01|\n",
      "|        10.5|        8|       302.0|       140|            17.0|         ford torino|   USA|         3449|1970-01-01|\n",
      "|        10.0|        8|       429.0|       198|            15.0|    ford galaxie 500|   USA|         4341|1970-01-01|\n",
      "|         9.0|        8|       454.0|       220|            14.0|    chevrolet impala|   USA|         4354|1970-01-01|\n",
      "|         8.5|        8|       440.0|       215|            14.0|   plymouth fury iii|   USA|         4312|1970-01-01|\n",
      "|        10.0|        8|       455.0|       225|            14.0|    pontiac catalina|   USA|         4425|1970-01-01|\n",
      "|         8.5|        8|       390.0|       190|            15.0|  amc ambassador dpl|   USA|         3850|1970-01-01|\n",
      "|        17.5|        4|       133.0|       115|            null|citroen ds-21 pallas|Europe|         3090|1970-01-01|\n",
      "|        11.5|        8|       350.0|       165|            null|chevrolet chevell...|   USA|         4142|1970-01-01|\n",
      "|        11.0|        8|       351.0|       153|            null|    ford torino (sw)|   USA|         4034|1970-01-01|\n",
      "|        10.5|        8|       383.0|       175|            null|plymouth satellit...|   USA|         4166|1970-01-01|\n",
      "|        11.0|        8|       360.0|       175|            null|  amc rebel sst (sw)|   USA|         3850|1970-01-01|\n",
      "|        10.0|        8|       383.0|       170|            15.0| dodge challenger se|   USA|         3563|1970-01-01|\n",
      "|         8.0|        8|       340.0|       160|            14.0|   plymouth cuda 340|   USA|         3609|1970-01-01|\n",
      "|         8.0|        8|       302.0|       140|            null|ford mustang boss...|   USA|         3353|1970-01-01|\n",
      "|         9.5|        8|       400.0|       150|            15.0|chevrolet monte c...|   USA|         3761|1970-01-01|\n",
      "+------------+---------+------------+----------+----------------+--------------------+------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cars_df = spark.read. \\\n",
    "    format(\"json\"). \\\n",
    "    option(\"inferSchema\", \"true\"). \\\n",
    "    option(\"mode\", \"failFast\"). \\\n",
    "    option(\"path\", \"data/cars\"). \\\n",
    "    load()\n",
    "\n",
    "cars_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316dbcf1-06be-4c56-bafd-687164658857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDFS\n",
    "# option(\"path\", \"hdfs://nn1home:8020/sources/cars\"). \\\n",
    "\n",
    "# FTP\n",
    "# option(\"path\", \"ftp://user:pwd/192.168.1.5/sources/cars\"). \\\n",
    "\n",
    "# S3\n",
    "# option(\"path\", s3://bucket-name/sources/cars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "460bb422-ca13-4c90-9d85-bd01f629222c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+------------+----------+----------------+--------------------+------+-------------+----------+\n",
      "|Acceleration|Cylinders|Displacement|Horsepower|Miles_per_Gallon|                Name|Origin|Weight_in_lbs|      Year|\n",
      "+------------+---------+------------+----------+----------------+--------------------+------+-------------+----------+\n",
      "|        12.0|        8|       307.0|       130|            18.0|chevrolet chevell...|   USA|         3504|      null|\n",
      "|        12.0|        8|       307.0|       130|            18.0|chevrolet chevell...|   USA|         3504|1970-01-01|\n",
      "|        11.5|        8|       350.0|       165|            15.0|   buick skylark 320|   USA|         3693|1970-01-01|\n",
      "|        11.0|        8|       318.0|       150|            18.0|  plymouth satellite|   USA|         3436|1970-01-01|\n",
      "|        12.0|        8|       304.0|       150|            16.0|       amc rebel sst|   USA|         3433|1970-01-01|\n",
      "|        10.5|        8|       302.0|       140|            17.0|         ford torino|   USA|         3449|1970-01-01|\n",
      "|        10.0|        8|       429.0|       198|            15.0|    ford galaxie 500|   USA|         4341|1970-01-01|\n",
      "|         9.0|        8|       454.0|       220|            14.0|    chevrolet impala|   USA|         4354|1970-01-01|\n",
      "|         8.5|        8|       440.0|       215|            14.0|   plymouth fury iii|   USA|         4312|1970-01-01|\n",
      "|        10.0|        8|       455.0|       225|            14.0|    pontiac catalina|   USA|         4425|1970-01-01|\n",
      "|         8.5|        8|       390.0|       190|            15.0|  amc ambassador dpl|   USA|         3850|1970-01-01|\n",
      "|        17.5|        4|       133.0|       115|            null|citroen ds-21 pallas|Europe|         3090|1970-01-01|\n",
      "|        11.5|        8|       350.0|       165|            null|chevrolet chevell...|   USA|         4142|1970-01-01|\n",
      "|        11.0|        8|       351.0|       153|            null|    ford torino (sw)|   USA|         4034|1970-01-01|\n",
      "|        10.5|        8|       383.0|       175|            null|plymouth satellit...|   USA|         4166|1970-01-01|\n",
      "|        11.0|        8|       360.0|       175|            null|  amc rebel sst (sw)|   USA|         3850|1970-01-01|\n",
      "|        10.0|        8|       383.0|       170|            15.0| dodge challenger se|   USA|         3563|1970-01-01|\n",
      "|         8.0|        8|       340.0|       160|            14.0|   plymouth cuda 340|   USA|         3609|1970-01-01|\n",
      "|         8.0|        8|       302.0|       140|            null|ford mustang boss...|   USA|         3353|1970-01-01|\n",
      "|         9.5|        8|       400.0|       150|            15.0|chevrolet monte c...|   USA|         3761|1970-01-01|\n",
      "+------------+---------+------------+----------+----------------+--------------------+------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cars_df_v2 = spark.read. \\\n",
    "    format(\"json\"). \\\n",
    "    options(mode=\"failFast\", path=\"data/cars\", inferSchema=\"true\"). \\\n",
    "    load()\n",
    "\n",
    "cars_df_v2.show()\n",
    "\n",
    "         # /sources/cars\n",
    "# 10.1.1.1 node1 -> block1     S3 NETWORK                             -> partition1 -> task1\n",
    "# 10.1.1.2 node2 -> block2 -> Spark Driver -> Name Node -> ip adress -> partition2 -> task2\n",
    "# 10.1.1.3 node3 -> block2                                           -> parttion3 -> task3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "749a63f5-e4f6-409c-b073-c6b3f3f7558d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+------------+----------+----------------+-------------------------+------+-------------+----------+\n",
      "|Acceleration|Cylinders|Displacement|Horsepower|Miles_per_Gallon|Name                     |Origin|Weight_in_lbs|Year      |\n",
      "+------------+---------+------------+----------+----------------+-------------------------+------+-------------+----------+\n",
      "|12.0        |8        |307.0       |130       |18.0            |chevrolet chevelle malibu|USA   |3504         |null      |\n",
      "|12.0        |8        |307.0       |130       |18.0            |chevrolet chevelle malibu|USA   |3504         |1970-01-01|\n",
      "|11.5        |8        |350.0       |165       |15.0            |buick skylark 320        |USA   |3693         |1970-01-01|\n",
      "|11.0        |8        |318.0       |150       |18.0            |plymouth satellite       |USA   |3436         |1970-01-01|\n",
      "|12.0        |8        |304.0       |150       |16.0            |amc rebel sst            |USA   |3433         |1970-01-01|\n",
      "|10.5        |8        |302.0       |140       |17.0            |ford torino              |USA   |3449         |1970-01-01|\n",
      "|10.0        |8        |429.0       |198       |15.0            |ford galaxie 500         |USA   |4341         |1970-01-01|\n",
      "|9.0         |8        |454.0       |220       |14.0            |chevrolet impala         |USA   |4354         |1970-01-01|\n",
      "|8.5         |8        |440.0       |215       |14.0            |plymouth fury iii        |USA   |4312         |1970-01-01|\n",
      "|10.0        |8        |455.0       |225       |14.0            |pontiac catalina         |USA   |4425         |1970-01-01|\n",
      "+------------+---------+------------+----------+----------------+-------------------------+------+-------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cars_df.show(10, False)\n",
    "\n",
    "cars_df.\\\n",
    "    repartition(3). \\\n",
    "    write. \\\n",
    "    mode(\"overwrite\"). \\\n",
    "    option(\"compression\", \"snappy\"). \\\n",
    "    parquet(\"../sources/parquet\")\n",
    "\n",
    "\n",
    "#    repartition(3). \\\n",
    "#    write. \\\n",
    "#    partitionBy(\"Year\"). \\\n",
    "\n",
    "#    repartition(\"Year\"). \\\n",
    "#    write. \\\n",
    "#    partitionBy(\"Year\"). \\\n",
    "\n",
    "    \n",
    "# A lot of small files problem\n",
    "# repartition(3) => round robin\n",
    "# repartition(col(\"field\")) => hash partitioning\n",
    "# repartition(3) + partitionBy(\"Year\") NOT GOOD\n",
    "# repartition(col(\"field\")) + partitionBy(\"Year\") GOOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6caab552-a544-41c0-ad7c-4c2ed756c8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|Lorem ipsum dolor...|\n",
      "|                    |\n",
      "|Vestibulum sed di...|\n",
      "|                    |\n",
      "|Aliquam vestibulu...|\n",
      "|                    |\n",
      "|Fusce in accumsan...|\n",
      "|                    |\n",
      "|Nullam ultrices q...|\n",
      "|                    |\n",
      "|Nam tempor nisi a...|\n",
      "|                    |\n",
      "|Nunc vulputate ac...|\n",
      "|                    |\n",
      "|Proin blandit ero...|\n",
      "|                    |\n",
      "|Ut bibendum ac du...|\n",
      "|                    |\n",
      "|Vestibulum ac var...|\n",
      "|                    |\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# stocks_df.write.save(\"data/stocks_parquet\")\n",
    "\n",
    "# each row is a value in a DF with a SINGLE column (\"value\")\n",
    "text_df = spark.read.text(\"data/lipsum\")\n",
    "text_df.show()\n",
    "\n",
    "# !!!!!!!!!!!!! DIFFERENCE between \n",
    "# saveAsTable() \n",
    "# write + save()\n",
    "# parquet() + ???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b1176c-e28c-40b6-bc8d-cdb8782f7fdd",
   "metadata": {},
   "source": [
    "# data_formats_json_avro_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdd3c700-6c06-4585-8b40-01b2e42dfeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+--------+---+\n",
      "|State|Gender|Year|Name    |Cnt|\n",
      "+-----+------+----+--------+---+\n",
      "|IN   |F     |1910|Mary    |619|\n",
      "|IN   |F     |1910|Helen   |324|\n",
      "|IN   |F     |1910|Ruth    |238|\n",
      "|IN   |F     |1910|Dorothy |215|\n",
      "|IN   |F     |1910|Mildred |200|\n",
      "|IN   |F     |1910|Margaret|196|\n",
      "|IN   |F     |1910|Thelma  |137|\n",
      "|IN   |F     |1910|Edna    |113|\n",
      "|IN   |F     |1910|Martha  |112|\n",
      "|IN   |F     |1910|Hazel   |108|\n",
      "+-----+------+----+--------+---+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- State: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Cnt: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state_names_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"data/statenames\")\n",
    "\n",
    "state_names_df.show(10, False)\n",
    "state_names_df.printSchema()\n",
    "\n",
    "state_names_df \\\n",
    "    .coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"data/target/statenames_parquet\")\n",
    "\n",
    "# Parquet = binary data, high compression, low CPU usage, very fast\n",
    "# also contains the schema\n",
    "# the default data format in Spark\n",
    "\n",
    "\n",
    "# Problem: which format best for read, write, storage size()\n",
    "\n",
    "# TODO - ADD COMPARING DATA FORMATS FOR CLUSTER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8750ec57-33b6-43c5-9ffa-e4ae46aa9aec",
   "metadata": {},
   "source": [
    "# jdbc_postgres_oracle\n",
    "# WORK WITH JDBC FROM PyCharm projects https://github.com/vadopolski/eas-017-RDD-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd45d80f-a814-46e3-80eb-efccf8af5aaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o76.jdbc.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:39)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:33)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\n\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:294)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m employees_pruned \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m(select e.first_name, e.last_name, e.hire_date from public.employees e where e.gender = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) as new_emp\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# 10101        99999\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 10102        99998\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 10103        10103\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memployees\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDBPARAMS\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:529\u001b[0m, in \u001b[0;36mDataFrameReader.jdbc\u001b[0;34m(self, url, table, column, lowerBound, upperBound, numPartitions, predicates, properties)\u001b[0m\n\u001b[1;32m    527\u001b[0m     jpredicates \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mtoJArray(gateway, gateway\u001b[38;5;241m.\u001b[39mjvm\u001b[38;5;241m.\u001b[39mjava\u001b[38;5;241m.\u001b[39mlang\u001b[38;5;241m.\u001b[39mString, predicates)\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mjdbc(url, table, jpredicates, jprop))\n\u001b[0;32m--> 529\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjprop\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o76.jdbc.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:39)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:33)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\n\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:294)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "driver = \"org.postgresql.Driver\"\n",
    "url = \"jdbc:postgresql://localhost:5432/spark\"\n",
    "user = \"docker\"\n",
    "password = \"docker\"\n",
    "\n",
    "DBPARAMS = {\n",
    "    \"user\": user,\n",
    "    \"password\": password,\n",
    "    \"driver\": driver\n",
    "}\n",
    "\n",
    "\n",
    "# \n",
    "# PROBLEM HOW TO READ/WRITE DATA FROM JDBC\n",
    "\n",
    "\n",
    "# NOT WORKING HERE\n",
    "# Py4JJavaError: An error occurred while calling o73.jdbc.\n",
    "# : java.lang.ClassNotFoundException: org.postgresql.Driver\n",
    "# \tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n",
    "\n",
    "employees = \"public.employees\"\n",
    "employees_pruned = \"\"\"(select e.first_name, e.last_name, e.hire_date from public.employees e where e.gender = 'F') as new_emp\"\"\"\n",
    "\n",
    "# 10101        99999\n",
    "# 10102        99998\n",
    "# 10103        10103\n",
    "\n",
    "df = spark.read.jdbc(url=url, table=employees, properties=DBPARAMS)\n",
    "\n",
    "# print(\"GET NUM PARTITIONS\")\n",
    "# print(df.rdd.getNumPartitions())\n",
    "\n",
    "# df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d1a767-52bc-4548-b19e-39618528e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.printSchema()\n",
    "# df.agg(F.max(F.col(\"emp_no\")), F.min(F.col(\"emp_no\"))).show()\n",
    "\n",
    "\n",
    "df = spark.read.jdbc(url=url, table=\"public.employees\", properties=DBPARAMS,\n",
    "                     column=\"emp_no\", lowerBound = 10010, upperBound = 499990, numPartitions = 10)\n",
    "\n",
    "# lowerBound = 10010\n",
    "# upperBound = 499990\n",
    "#\n",
    "# ex1 => part1 => select * from public.employees e where e.emp_num > x and e.emp_num\n",
    "# ex2 => part2 =\n",
    "\n",
    "pred = [\"gender = 'F'\", \"gender = 'M'\", \"gender = 'M'\"]\n",
    "# be carefully with borders\n",
    "pred2 = [\"emp_no > 10010 and emp_no <= 50000\", \"emp_no >= 50000 and emp_no <= 100000\"]\n",
    "\n",
    "df = spark.read.jdbc(url=url, table=\"public.employees\", properties=DBPARAMS, predicates =pred)\n",
    "df.show()\n",
    "\n",
    "# lowerBound = 10010,\n",
    "# upperBound = 499990,\n",
    "# numPartitions = 20,\n",
    "\n",
    "# Killer joins => optimised UDF\n",
    "\n",
    "# print(\"GET NUM PARTITIONS\")\n",
    "# print(df.rdd.getNumPartitions())\n",
    "#\n",
    "# df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368cd4b5-0c62-4510-8570-91dbba526437",
   "metadata": {},
   "outputs": [],
   "source": [
    "employees_df = spark.read. \\\n",
    "    format(\"jdbc\"). \\\n",
    "    option(\"driver\", driver). \\\n",
    "    option(\"url\", url). \\\n",
    "    option(\"user\", user). \\\n",
    "    option(\"password\", password). \\\n",
    "    option(\"dbtable\", \"public.employees\"). \\\n",
    "    load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dd091b-9ee4-4234-a4af-2fec2d779598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# department_df = spark.read (dept_no, dept_name) // 200\n",
    "#\n",
    "# employees_df. \\\n",
    "#     groupBy(\"dept_no\"). \\\n",
    "#     count(). \\\n",
    "#     join(department_df, col(\"dept_no\") = col(\"dept_no\"),  \"inner\")\n",
    "\n",
    "# Solution1 UDF\n",
    "#\n",
    "\n",
    "print(\"GET NUM PARTITIONS\")\n",
    "print(employees_df.rdd.getNumPartitions())\n",
    "\n",
    "\n",
    "employees_df.show()\n",
    "\n",
    "employees_df.write.bucketBy(10, \"emp_no\").sortBy(\"emp_no\").mode(\"overwrite\").saveAsTable(\"employee_bucketed\")\n",
    "# employees_df.write.mode(\"overwrite\").save() Parquet\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830a46c8-13c7-4aaa-bd48-2aca1dab4f1b",
   "metadata": {},
   "source": [
    "# queue_kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7fba04-832f-4387-a42a-85fe32edcd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"page\", StringType())\n",
    "])\n",
    "\n",
    "# TODO NOT WORKING HERE TOO\n",
    "\n",
    "\n",
    "# source_batch_df = spark.read\\\n",
    "#     .format(\"kafka\")\\\n",
    "#     .option(\"kafka.bootstrap.servers\", \"localhost:29092\")\\\n",
    "#     .option(\"subscribe\", \"input\")\\\n",
    "#     .load()\n",
    "#\n",
    "# print(source_batch_df.isStreaming)\n",
    "#\n",
    "# source_batch_df.show()\n",
    "\n",
    "\n",
    "source_streaming_df = spark.readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:29092\")\\\n",
    "    .option(\"subscribe\", \"input\")\\\n",
    "    .load()\n",
    "\n",
    "print(source_streaming_df.isStreaming)\n",
    "\n",
    "typed_source_streaming_df = source_streaming_df.\\\n",
    "    select(expr(\"cast(value as string) as actualValue\")).\\\n",
    "    select(from_json(col(\"actualValue\"), schema).alias(\"page\")).\\\n",
    "    selectExpr(\"page.timestamp as timestamp\", \"page.page as page\").\\\n",
    "    select(date_format(to_timestamp(col(\"timestamp\"), \"dd-MM-yyyy HH:mm:ss:SSS\"), \"HH:mm:ss:SSS\").alias(\"time\"),col(\"page\")\n",
    "  )\n",
    "\n",
    "source_streaming_df.\\\n",
    "    writeStream.\\\n",
    "    outputMode(\"append\").\\\n",
    "    foreachBatch(lambda b, l: b.show).\\\n",
    "    trigger(processingTime='3 seconds').\\\n",
    "    start().\\\n",
    "    awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c46a91-41a2-49da-aa9b-35c110017aa7",
   "metadata": {},
   "source": [
    "Exercise: read the movies DF, then write it as\n",
    "- tab-separated \"CSV\"\n",
    "- parquet\n",
    "- table \"public.movies\" in the Postgres DB\n",
    "\n",
    "Exercise #2: find a way to read the people-1m dataFrame. Then write it as JSON."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
