{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93619a9e-3407-4736-b531-f628b35a0f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef53eedf-a69b-46ea-afdc-358f8b35700e",
   "metadata": {},
   "source": [
    "Spark cluster parallelism \n",
    "executors_num\n",
    "memory_per_ex\n",
    "cores_per_execut\n",
    "s = executors_num * cores_per_execut = 400 slotes\n",
    "20 block => 20 slotes ~ 95%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435dce30-e244-49b2-860a-46fc2f1467a7",
   "metadata": {},
   "source": [
    "# 1. HOW TO CREATE RDD\n",
    "# we can build RDDs out of local collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae08e150-e45c-4f30-8175-880e2059d4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = range(1, 1000000)\n",
    "numbers_parent_rdd = sc.parallelize(numbers, 4)\n",
    "\n",
    "# Dependency: numbers_rdd => numbers_rdd_2\n",
    "# Linage: partition: block => numbers_rdd => numbers_rdd_2\n",
    "numbers_parent_rdd.take(10)\n",
    "\n",
    "numbers_parent_rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a94ed5-704e-4588-9d53-4ac5d2d3c373",
   "metadata": {},
   "source": [
    "How to read a file in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9abf843d-8b55-4198-93e0-04ffd9275e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['AAPL', 'Jan 1 2000', '25.94'],\n",
       " ['AAPL', 'Feb 1 2000', '28.66'],\n",
       " ['AAPL', 'Mar 1 2000', '33.95'],\n",
       " ['AAPL', 'Apr 1 2000', '31.01'],\n",
       " ['AAPL', 'May 1 2000', '21'],\n",
       " ['AAPL', 'Jun 1 2000', '26.19'],\n",
       " ['AAPL', 'Jul 1 2000', '25.41'],\n",
       " ['AAPL', 'Aug 1 2000', '30.47'],\n",
       " ['AAPL', 'Jun 1 2004', '16.27'],\n",
       " ['AAPL', 'Jul 1 2004', '16.17']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks_rdd_v2 = sc.textFile(\"data/stocks/aapl.csv\"). \\\n",
    "    map(lambda row: row.split(\",\")). \\\n",
    "    filter(lambda tokens: float(tokens[2]) > 15)\n",
    "\n",
    "stocks_rdd_v2.take(10)\n",
    "\n",
    "# protected def getPartitions: Array[Partition]\n",
    "# Partition -> adress block of file -> \n",
    "# aapl.csv\n",
    "# block 1  -> Partition\n",
    "# block 2  \n",
    "# block 3\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f042d2e-0888-425c-af23-444e69ff99c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['25.94',\n",
       " '28.66',\n",
       " '33.95',\n",
       " '31.01',\n",
       " '21',\n",
       " '26.19',\n",
       " '25.41',\n",
       " '30.47',\n",
       " '12.88',\n",
       " '9.78']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read from a DF\n",
    "stocks_df = spark.read.csv(\"data/stocks\"). \\\n",
    "    withColumnRenamed(\"_c0\", \"company\"). \\\n",
    "    withColumnRenamed(\"_c1\", \"date\"). \\\n",
    "    withColumnRenamed(\"_c2\", \"price\")\n",
    "\n",
    "stocks_rdd_v3 = stocks_df.rdd\n",
    "\n",
    "prices_rdd = stocks_rdd_v3.map(lambda row: row.price)\n",
    "prices_rdd.toDebugString()\n",
    "prices_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "409efef6-edbf-46c8-986d-2358116235b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(company='AAPL', date='Jan 1 2000', price='25.94'),\n",
       " Row(company='AAPL', date='Feb 1 2000', price='28.66'),\n",
       " Row(company='AAPL', date='Mar 1 2000', price='33.95'),\n",
       " Row(company='AAPL', date='Apr 1 2000', price='31.01'),\n",
       " Row(company='AAPL', date='May 1 2000', price='21'),\n",
       " Row(company='AAPL', date='Jun 1 2000', price='26.19'),\n",
       " Row(company='AAPL', date='Jul 1 2000', price='25.41'),\n",
       " Row(company='AAPL', date='Aug 1 2000', price='30.47'),\n",
       " Row(company='AAPL', date='Sep 1 2000', price='12.88'),\n",
       " Row(company='AAPL', date='Oct 1 2000', price='9.78')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD to DF\n",
    "# condition: the RDD must contain Spark Rows (data structures conforming to a schema)\n",
    "stocks_df_v2 = spark.createDataFrame(stocks_rdd_v3)\n",
    "stocks_df_v2.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33257034-53b5-464f-a6eb-1888df2f5814",
   "metadata": {},
   "source": [
    "    Use cases for RDDs\n",
    "    - the computations that cannot work on DFs/Spark SQL API\n",
    "    - very custom perf optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48c67ffe-ee90-41dc-87e2-9258e7b31648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AAPL', 'AMZN', 'MSFT', 'IBM', 'GOOG']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD transformations\n",
    "# map, filter, flatMap\n",
    "\n",
    "# distinct\n",
    "company_names_rdd = stocks_rdd_v3 \\\n",
    "    .map(lambda row: row.company) \\\n",
    "    .distinct()\n",
    "company_names_rdd.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d09cf766-1458-4285-a664-2fe6b1feda3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# counting\n",
    "total_entries = stocks_rdd_v3.count()  # action - the RDD must be evaluated\n",
    "total_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f724343a-67b7-41c8-b847-072d6a4280bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223.02\n",
      "7.07\n"
     ]
    }
   ],
   "source": [
    "# min and max\n",
    "aapl_stocks_rdd = stocks_rdd_v3 \\\n",
    "    .filter(lambda row: row.company == \"AAPL\") \\\n",
    "    .map(lambda row: float(row.price))\n",
    "\n",
    "\n",
    "# narrow transformation\n",
    "# filter: RDD => RDD\n",
    "# map: RDD => RDD\n",
    "\n",
    "# action\n",
    "# count: RDD => integer\n",
    "# max: RDD => max\n",
    "\n",
    "\n",
    "\n",
    "max_aapl = aapl_stocks_rdd.max()\n",
    "min_aapl = aapl_stocks_rdd.min()\n",
    "print(max_aapl)\n",
    "print(min_aapl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77f1b93c-946b-45ad-ba93-6eae9dbbcd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7961.850000000001\n"
     ]
    }
   ],
   "source": [
    "# reduce ACTION\n",
    "sum_prices = aapl_stocks_rdd \\\n",
    "    .reduce(lambda x, y: x + y)  # can use ANY Python function here  1,2,3,4 => 1+2 = 3 + 3 = 6 + 4\n",
    "print(sum_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcce2031-9c25-407c-be5d-381f816b8b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AAPL', <pyspark.resultiterable.ResultIterable at 0x2119a967110>),\n",
       " ('AMZN', <pyspark.resultiterable.ResultIterable at 0x2119a9d4990>),\n",
       " ('MSFT', <pyspark.resultiterable.ResultIterable at 0x2119aa03b10>),\n",
       " ('IBM', <pyspark.resultiterable.ResultIterable at 0x2119aa13810>),\n",
       " ('GOOG', <pyspark.resultiterable.ResultIterable at 0x2119aa23550>)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grouping\n",
    "grouped_stocks_rdd = stocks_rdd_v3 \\\n",
    "    .groupBy(lambda row: row.company)  # can use ANY grouping criterion as a Python function\n",
    "# grouping is expensive - involves a shuffle\n",
    "grouped_stocks_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ebaa4a8-17a9-4568-8171-c455df01f47e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4174633332.py, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[12], line 16\u001b[1;36m\u001b[0m\n\u001b[1;33m    .repartition(2) # shuffle\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# partitioning\n",
    "repartitioned_stocks_rdd = stocks_rdd_v3.repartition(4).coalesce(2) #.repartition(row.company)  #.coalesce(2)\n",
    "\n",
    "repartitioned_stocks_rdd.getNumPartitions()\n",
    "# repartitioned_stocks_rdd.getNumPartitions()\n",
    "\n",
    "# RDD\n",
    "#  part1 => |||||| 20           \n",
    "#  part2 => |||||||||||||| 40   \n",
    "#  part3 => ||||| 10            \n",
    "#  part4 => ||||| 10       \n",
    "\n",
    "\n",
    "# nums of executor * nums of cores = max_parallesim = number slot = 100\n",
    "#  I/O or filter, map, ....\n",
    ".repartition(2) # shuffle\n",
    "#  part1 => ||||| 40\n",
    "#  part2 => ||||| 40\n",
    "\n",
    "# 50 block => 50 partitions => 50 tasks \n",
    "# 50 slot will \n",
    "# 50 slots will \n",
    "\n",
    "# 1 GB ~  size of each row * count\n",
    "# size of each row = 1MB  => 200 MB\n",
    "\n",
    ".coalesce(5) # 5 parttions => \n",
    "# part1 => |||||| 20 + |||||||||||||| 40 => 60\n",
    "# part2 => ||||| 10 +   ||||| 10 => 20\n",
    "\n",
    ".coalesce(1)\n",
    "\n",
    "\n",
    ".repartition(col(\"company\")) # 200\n",
    "#  part1 => |||||| 20           \n",
    "#  part2 => |||||||||||||| 40   \n",
    "#  part3 => ||||| 10            \n",
    "#  part4 => ||||| 10       \n",
    "\n",
    "\n",
    "\n",
    "repartitioned_stocks_rdd.join()\n",
    "\n",
    "repartitioned_stocks_rdd.group()\n",
    "\n",
    "repartitioned_stocks_rdd.group()\n",
    "\n",
    "# getHash from \"company\" => number - 12312312312321 % 200 => rest 1-200 => 1\n",
    "# part1 => | => groups with a equal  \"company\"  \"Coca Cola\"\n",
    "# .....\n",
    "# part36 => ||| => \"BMW\"\n",
    "# .....\n",
    "# part200  => |||||||| \"VW\"\n",
    "\n",
    "# groupBy = \"company\"\n",
    "# join = \"company\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TODO: SLIDE\n",
    "# .repartition(30)  # involves a shuffle\n",
    "# involves a shuffle\n",
    "#  .repartition(5) 100\n",
    "#  part1 => |||||| 20           20 2  =>\n",
    "#  part2 => |||||||||||||| 40   20 2  => |||||||||||||| 40 + |||||| 20 = 60\n",
    "#  part3 => ||||| 10            20 2\n",
    "#  part4 => |||||||||| 30       20 2  => |||||||||| 30 + ||||| 10 = 40\n",
    "#  part5 =>                     20 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315d5c70-c2fa-498d-8ee5-7905ed5d0fd4",
   "metadata": {},
   "source": [
    "Exercises\n",
    "\n",
    "1. Read the movies dataset as an RDD\n",
    "\n",
    "2. Show the distinct genres as an RDD\n",
    "\n",
    "3. Print all the movies in the Drama genre with IMDB rating > 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f2e63f7-5bdf-4b24-825a-0c4121a14a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Creative_Type=None, Director=None, Distributor='Gramercy', IMDB_Rating=6.1, IMDB_Votes=1071, MPAA_Rating='R', Major_Genre=None, Production_Budget=8000000, Release_Date='12-Jun-98', Rotten_Tomatoes_Rating=None, Running_Time_min=None, Source=None, Title='The Land Girls', US_DVD_Sales=None, US_Gross=146083, Worldwide_Gross=146083),\n",
       " Row(Creative_Type=None, Director=None, Distributor='Strand', IMDB_Rating=6.9, IMDB_Votes=207, MPAA_Rating='R', Major_Genre='Drama', Production_Budget=300000, Release_Date='7-Aug-98', Rotten_Tomatoes_Rating=None, Running_Time_min=None, Source=None, Title='First Love, Last Rites', US_DVD_Sales=None, US_Gross=10876, Worldwide_Gross=10876),\n",
       " Row(Creative_Type=None, Director=None, Distributor='Lionsgate', IMDB_Rating=6.8, IMDB_Votes=865, MPAA_Rating=None, Major_Genre='Comedy', Production_Budget=250000, Release_Date='28-Aug-98', Rotten_Tomatoes_Rating=None, Running_Time_min=None, Source=None, Title='I Married a Strange Person', US_DVD_Sales=None, US_Gross=203134, Worldwide_Gross=203134),\n",
       " Row(Creative_Type=None, Director=None, Distributor='Fine Line', IMDB_Rating=None, IMDB_Votes=None, MPAA_Rating=None, Major_Genre='Comedy', Production_Budget=300000, Release_Date='11-Sep-98', Rotten_Tomatoes_Rating=13, Running_Time_min=None, Source=None, Title=\"Let's Talk About Sex\", US_DVD_Sales=None, US_Gross=373615, Worldwide_Gross=373615),\n",
       " Row(Creative_Type='Contemporary Fiction', Director=None, Distributor='Trimark', IMDB_Rating=3.4, IMDB_Votes=165, MPAA_Rating='R', Major_Genre='Drama', Production_Budget=1000000, Release_Date='9-Oct-98', Rotten_Tomatoes_Rating=62, Running_Time_min=None, Source='Original Screenplay', Title='Slam', US_DVD_Sales=None, US_Gross=1009819, Worldwide_Gross=1087521)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df = spark.read.json(\"data/movies\")\n",
    "movies_rdd = movies_df.rdd\n",
    "\n",
    "movies_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc63eaf-93ff-4e38-8277-57d64f8f883a",
   "metadata": {},
   "source": [
    "# 2. HOW TO SAVE AND PERSIST RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a1df60a8-27a0-4bcd-851b-ee662773780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "ints = sc.parallelize(r, 4).coalesce(2)\n",
    "\n",
    "ints.getNumPartitions()\n",
    "\n",
    "ints.saveAsTextFile(\"data/output/ints\")\n",
    "\n",
    "\n",
    "# ints = sc.parallelize(r).coalesce(1)\n",
    "# ints.coalesce(2) \\\n",
    "#     .saveAsTextFile(\"data/output/ints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77352e87-0b25-4182-81f0-467331037c88",
   "metadata": {},
   "source": [
    "# 3. HOW TO GROUP AND JOIN RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8b4956d6-0bcd-48ca-a4ff-def362d69931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ivan', 240), ('Petr', 39), ('Elena', 290), ('Elena', 300)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(\"Ivan\", 240), (\"Petr\", 39), (\"Elena\", 290), (\"Elena\", 300)]\n",
    "codeRows = sc.parallelize(data)\n",
    "codeRows.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7ef57bf9-bf14-4682-943e-de84b8ef7d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ivan', 240), ('Petr', 39), ('Elena', 590)]\n"
     ]
    }
   ],
   "source": [
    "# how to reduce\n",
    "reduced = codeRows.reduceByKey(lambda x, y: x + y)\n",
    "print(reduced.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e8920c4c-10ee-48da-992f-9f72c81f70f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ivan', 240), ('Petr', 39), ('Elena', 300)]\n"
     ]
    }
   ],
   "source": [
    "# how to deduplicate\n",
    "deduplicated = codeRows.reduceByKey(lambda x, y: x if (x > y) else y)\n",
    "print(deduplicated.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "179d5758-6230-48c8-b88c-ad3270f2eeb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ivan', 1240), ('Petr', 1039), ('Elena', 1590)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to fold by key\n",
    "folded = codeRows.foldByKey(1000, lambda x, y: x + y)\n",
    "\n",
    "\n",
    "# TODO Sliding\n",
    "#     How to foldByKey works\n",
    "#     part1 (k1:2, k2:2, k3:2, k1:2) => shufle => reduce (k1:2, k1:2, k1:2) => k1:6\n",
    "#     part2 (k2:2, k2:2, k3:2, k1:2) shufle => (k2:2, k2:2, k2:2) => k2:6, (k3:2, k3:2) => k3:4\n",
    "\n",
    "folded.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "615665f4-a0a4-4de7-9eea-1574cd829494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ivan', 1240), ('Petr', 1039), ('Elena', 1590)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregated\n",
    "aggregated = codeRows.aggregateByKey(1000, lambda x, y: x + y, lambda x, y: x + y)\n",
    "aggregated.collect()\n",
    "\n",
    "#     How to aggregateByKey works, shuffle less\n",
    "#     part1 (k1:2, k2:2, k3:2, k1:2) => (k1:4, k2:2, k3:2) =>  shuffle => (k1:4, k1:2) => k1:6\n",
    "#     part2 (k2:2, k2:2, k3:2, k1:2) => (k1:2, k2:4, k3:2) => shuffle => (k2:4, k2:2) => k2:6, (k3:2, k3:2) => k3:4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "06cdbceb-07ac-4b88-83d9-582b30bc82ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Ivan', <pyspark.resultiterable.ResultIterable object at 0x7fc7ee232430>), ('Petr', <pyspark.resultiterable.ResultIterable object at 0x7fc7ee2322b0>), ('Elena', <pyspark.resultiterable.ResultIterable object at 0x7fc7ee2323a0>)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'(1) PythonRDD[290] at collect at /tmp/ipykernel_109/2934942739.py:3 []\\n |  MapPartitionsRDD[289] at mapPartitions at PythonRDD.scala:145 []\\n |  ShuffledRDD[288] at partitionBy at NativeMethodAccessorImpl.java:0 []\\n +-(1) PairwiseRDD[287] at groupByKey at /tmp/ipykernel_109/2934942739.py:2 []\\n    |  PythonRDD[286] at groupByKey at /tmp/ipykernel_109/2934942739.py:2 []\\n    |  ParallelCollectionRDD[235] at readRDDFromFile at PythonRDD.scala:274 []'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # groupByKey works\n",
    "grouped = codeRows.groupByKey()\n",
    "# TODO show the inner array\n",
    "print(grouped.collect())\n",
    "\n",
    "grouped.toDebugString().decode(\"utf-8\")\n",
    "\n",
    "# b'(1) PythonRDD[19] at collect at C:/Users/VOpolskiy/PycharmProjects/another/eas-017-RDD-py/lection/01-RDD.py:208 []\\n |\n",
    "# MapPartitionsRDD[18] at mapPartitions at PythonRDD.scala:145 []\\n |\n",
    "# ShuffledRDD[17] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
    "# \\n +-(1) PairwiseRDD[16] at groupByKey at C:/Users/VOpolskiy/PycharmProjects/another/eas-017-RDD-py/lection/01-RDD.py:207 []\n",
    "# \\n    |  PythonRDD[15] at groupByKey at C:/Users/VOpolskiy/PycharmProjects/another/eas-017-RDD-py/lection/01-RDD.py:207 []\n",
    "# \\n    |  ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274 []'\n",
    "# # Don't forget about joins with preferred languages\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3191ca-ab7a-49ad-a98d-ec94b144dd36",
   "metadata": {},
   "source": [
    "# Joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "272e6f0e-8461-4465-b2db-39c280a835c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ivan', 'Java'), ('Elena', 'Scala'), ('Petr', 'Scala')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profileData = [(\"Ivan\", \"Java\"), (\"Elena\", \"Scala\"), (\"Petr\", \"Scala\")]\n",
    "programmerProfiles = sc.parallelize(profileData)\n",
    "programmerProfiles.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "64cfced5-276d-4b22-9828-c06355168c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[202] at RDD at PythonRDD.scala:53 []\n",
      " |  MapPartitionsRDD[201] at mapPartitions at PythonRDD.scala:158 []\n",
      " |  ShuffledRDD[200] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      " +-(2) PairwiseRDD[199] at join at C:\\Users\\kokhrime\\AppData\\Local\\Temp\\ipykernel_8112\\3472480033.py:3 []\n",
      "    |  PythonRDD[198] at join at C:\\Users\\kokhrime\\AppData\\Local\\Temp\\ipykernel_8112\\3472480033.py:3 []\n",
      "    |  UnionRDD[197] at union at NativeMethodAccessorImpl.java:0 []\n",
      "    |  PythonRDD[195] at RDD at PythonRDD.scala:53 []\n",
      "    |  ParallelCollectionRDD[194] at readRDDFromFile at PythonRDD.scala:287 []\n",
      "    |  PythonRDD[196] at RDD at PythonRDD.scala:53 []\n",
      "    |  ParallelCollectionRDD[178] at readRDDFromFile at PythonRDD.scala:287 []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Elena', ('Scala', 290)),\n",
       " ('Elena', ('Scala', 300)),\n",
       " ('Petr', ('Scala', 39)),\n",
       " ('Ivan', ('Java', 240))]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD joining possible for only tuples\n",
    "\n",
    "joined = programmerProfiles.join(codeRows)\n",
    "print(joined.toDebugString().decode(\"utf-8\"))\n",
    "joined.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "29713661-8b43-46d1-8a25-78e42fd66c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Elena',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x2119ceba650>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x2119cfa0b50>)),\n",
       " ('Petr',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x2119d098950>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x2119cf2e9d0>)),\n",
       " ('Ivan',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x2119d15ea10>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x2119d15fa50>))]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cogroup is fullouter join with dividing array\n",
    "\n",
    "data = [(\"Ivan\", 240), (\"Petr\", 39), (\"Elena\", 290), (\"Elena\", 300)]\n",
    "codeRows = sc.parallelize(data)\n",
    "codeRows = programmerProfiles.cogroup(codeRows)\n",
    "codeRows.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b6b9e990-a4e8-46f1-a408-ad796157f0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Petr',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x2119a9d7350>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x2119cdac8d0>)),\n",
       " ('Ivan',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x2119cfe96d0>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x2119d06b550>)),\n",
       " ('Elena',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x2119cc13010>,\n",
       "   <pyspark.resultiterable.ResultIterable at 0x2119d122ed0>))]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sorting\n",
    "programmerProfiles.cogroup(codeRows).sortByKey(False).collect()\n",
    "\n",
    "# TODO Write code to show inner arry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "477a5557-7f6b-40ae-9b76-b42bba22a5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== CountByKey\n",
      "defaultdict(<class 'int'>, {'Elena': 2, 'Petr': 1, 'Ivan': 1})\n"
     ]
    }
   ],
   "source": [
    "print(\"== CountByKey\")\n",
    "print(str(joined.countByKey()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6901654b-8484-4b21-8d07-5f40920e05e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Keys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Elena', 'Petr', 'Ivan']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# codeRows keys only\n",
    "print(\"== Keys\")\n",
    "codeRows.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9b5ac72e-fde9-4ad2-b85b-35d1b1938ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Value\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(<pyspark.resultiterable.ResultIterable at 0x2119d1fea10>,\n",
       "  <pyspark.resultiterable.ResultIterable at 0x2119d1fe110>),\n",
       " (<pyspark.resultiterable.ResultIterable at 0x2119d1ff210>,\n",
       "  <pyspark.resultiterable.ResultIterable at 0x2119d1fc110>),\n",
       " (<pyspark.resultiterable.ResultIterable at 0x2119d1d9710>,\n",
       "  <pyspark.resultiterable.ResultIterable at 0x2119d209090>)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print values only\n",
    "print(\"== Value\")\n",
    "codeRows.values().take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8563ac4-6c84-4332-830f-b2563f595d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Creative_Type=None, Director=None, Distributor='Gramercy', IMDB_Rating=6.1, IMDB_Votes=1071, MPAA_Rating='R', Major_Genre=None, Production_Budget=8000000, Release_Date='12-Jun-98', Rotten_Tomatoes_Rating=None, Running_Time_min=None, Source=None, Title='The Land Girls', US_DVD_Sales=None, US_Gross=146083, Worldwide_Gross=146083), Row(Creative_Type=None, Director=None, Distributor='Strand', IMDB_Rating=6.9, IMDB_Votes=207, MPAA_Rating='R', Major_Genre='Drama', Production_Budget=300000, Release_Date='7-Aug-98', Rotten_Tomatoes_Rating=None, Running_Time_min=None, Source=None, Title='First Love, Last Rites', US_DVD_Sales=None, US_Gross=10876, Worldwide_Gross=10876), Row(Creative_Type=None, Director=None, Distributor='Lionsgate', IMDB_Rating=6.8, IMDB_Votes=865, MPAA_Rating=None, Major_Genre='Comedy', Production_Budget=250000, Release_Date='28-Aug-98', Rotten_Tomatoes_Rating=None, Running_Time_min=None, Source=None, Title='I Married a Strange Person', US_DVD_Sales=None, US_Gross=203134, Worldwide_Gross=203134), Row(Creative_Type=None, Director=None, Distributor='Fine Line', IMDB_Rating=None, IMDB_Votes=None, MPAA_Rating=None, Major_Genre='Comedy', Production_Budget=300000, Release_Date='11-Sep-98', Rotten_Tomatoes_Rating=13, Running_Time_min=None, Source=None, Title=\"Let's Talk About Sex\", US_DVD_Sales=None, US_Gross=373615, Worldwide_Gross=373615), Row(Creative_Type='Contemporary Fiction', Director=None, Distributor='Trimark', IMDB_Rating=3.4, IMDB_Votes=165, MPAA_Rating='R', Major_Genre='Drama', Production_Budget=1000000, Release_Date='9-Oct-98', Rotten_Tomatoes_Rating=62, Running_Time_min=None, Source='Original Screenplay', Title='Slam', US_DVD_Sales=None, US_Gross=1009819, Worldwide_Gross=1087521)]\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"inferSchema\", \"true\").json(\"data/movies\")\n",
    "movies_rdd = df.rdd\n",
    "\n",
    "print(movies_rdd.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7cc40b40-ec27-4d04-bb2e-48b3fb95c91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 'Drama', 'Comedy', 'Musical', 'Thriller/Suspense']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_movies = movies_rdd.map(lambda row: row.Major_Genre).distinct()\n",
    "dist_movies.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2be1ee46-213d-4ef8-9120-89368cf5be94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Creative_Type=None, Director=None, Distributor='Strand', IMDB_Rating=6.9, IMDB_Votes=207, MPAA_Rating='R', Major_Genre='Drama', Production_Budget=300000, Release_Date='7-Aug-98', Rotten_Tomatoes_Rating=None, Running_Time_min=None, Source=None, Title='First Love, Last Rites', US_DVD_Sales=None, US_Gross=10876, Worldwide_Gross=10876), Row(Creative_Type=None, Director='Sidney Lumet', Distributor='United Artists', IMDB_Rating=8.9, IMDB_Votes=119101, MPAA_Rating=None, Major_Genre='Drama', Production_Budget=340000, Release_Date='13-Apr-57', Rotten_Tomatoes_Rating=None, Running_Time_min=None, Source='Original Screenplay', Title='12 Angry Men', US_DVD_Sales=None, US_Gross=0, Worldwide_Gross=0), Row(Creative_Type='Science Fiction', Director='Terry Gilliam', Distributor='Universal', IMDB_Rating=8.1, IMDB_Votes=169858, MPAA_Rating='R', Major_Genre='Drama', Production_Budget=29000000, Release_Date='27-Dec-95', Rotten_Tomatoes_Rating=None, Running_Time_min=None, Source='Based on Short Film', Title='Twelve Monkeys', US_DVD_Sales=None, US_Gross=57141459, Worldwide_Gross=168841459), Row(Creative_Type='Historical Fiction', Director=None, Distributor='Sony/Columbia', IMDB_Rating=7.0, IMDB_Votes=4099, MPAA_Rating='PG', Major_Genre='Drama', Production_Budget=4000000, Release_Date='9-Nov-72', Rotten_Tomatoes_Rating=57, Running_Time_min=None, Source='Based on Play', Title='1776', US_DVD_Sales=None, US_Gross=0, Worldwide_Gross=0), Row(Creative_Type='Contemporary Fiction', Director='Michael Polish', Distributor='Sony Pictures Classics', IMDB_Rating=7.1, IMDB_Votes=2810, MPAA_Rating='R', Major_Genre='Drama', Production_Budget=500000, Release_Date='30-Jul-99', Rotten_Tomatoes_Rating=77, Running_Time_min=None, Source='Original Screenplay', Title='Twin Falls Idaho', US_DVD_Sales=None, US_Gross=985341, Worldwide_Gross=1027228)]\n"
     ]
    }
   ],
   "source": [
    "# spark_dsl_only_df = col(\"Major_Genre\") == \"Drama\" && col(\"IMDB_Rating\") > 6\n",
    "python_lambda_rdd = lambda movie: (movie.Major_Genre == \"Drama\") and (movie.IMDB_Rating is not None and movie.IMDB_Rating > 6)\n",
    "\n",
    "s_movies = movies_rdd.filter(python_lambda_rdd)\n",
    "\n",
    "print(s_movies.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62a7c667-6c5a-4201-9f52-3e8e38a790f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(\"Ivan\", 240), (\"Petr\", 39), (\"Elena\", 290), (\"Elena\", 300)]\n",
    "codeRows = sc.parallelize(data)\n",
    "\n",
    "grouped = codeRows.groupByKey()\n",
    "\n",
    "grouped.toDebugString().decode(\"utf-8\")\n",
    "\n",
    "grouped.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd853d9-886d-4a36-8b5f-7ef58cfce7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
