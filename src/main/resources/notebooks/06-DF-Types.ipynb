{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f91a7af4-ee51-4133-b2e4-801c2f59ebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    appName(\"Joins\"). \\\n",
    "    master(\"local\"). \\\n",
    "    config(\"spark.jars\", \"data/jars/postgresql-42.2.19.jar\"). \\\n",
    "    getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45c60be9-f708-4e10-8f08-bd123cf13cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = spark.read.json(\"data/movies\")\n",
    "assert(movies_df.count() != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f595abb-822e-4e03-84a8-e589f4565e2f",
   "metadata": {},
   "source": [
    "# Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c169d127-ea7a-4b30-b77a-bc9f97a70f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+---+\n",
      "|Title                     |MOL|\n",
      "+--------------------------+---+\n",
      "|The Land Girls            |42 |\n",
      "|First Love, Last Rites    |42 |\n",
      "|I Married a Strange Person|42 |\n",
      "|Let's Talk About Sex      |42 |\n",
      "|Slam                      |42 |\n",
      "+--------------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# demo_literal_values\n",
    "\n",
    "meaning_of_life_df = movies_df.select(col(\"Title\"), lit(42).alias(\"MOL\"))\n",
    "meaning_of_life_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78128f61-520a-4ce5-82a9-23155867b74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-----------+\n",
      "|               Title|Major_Genre|IMDB_Rating|\n",
      "+--------------------+-----------+-----------+\n",
      "|        12 Angry Men|      Drama|        8.9|\n",
      "|      Twelve Monkeys|      Drama|        8.1|\n",
      "|    Twin Falls Idaho|      Drama|        7.1|\n",
      "|                Amen|      Drama|        7.4|\n",
      "|        Barry Lyndon|      Drama|        8.1|\n",
      "|      Before Sunrise|      Drama|        8.0|\n",
      "|The Best Years of...|      Drama|        8.2|\n",
      "|      The Big Parade|      Drama|        8.4|\n",
      "|     Boyz n the Hood|      Drama|        7.8|\n",
      "|De battre mon coe...|      Drama|        7.3|\n",
      "|The Birth of a Na...|      Drama|        7.1|\n",
      "|The Bridge on the...|      Drama|        8.4|\n",
      "|Born on the Fourt...|      Drama|        7.2|\n",
      "|The Bridges of Ma...|      Drama|        7.2|\n",
      "|          Braveheart|      Drama|        8.4|\n",
      "|    Chariots of Fire|      Drama|        7.3|\n",
      "|Cat on a Hot Tin ...|      Drama|        8.0|\n",
      "|    The Color Purple|      Drama|        7.7|\n",
      "|   Central do Brasil|      Drama|        8.0|\n",
      "|             Dayereh|      Drama|        7.3|\n",
      "+--------------------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# demo_booleans\n",
    "\n",
    "drama_filter = movies_df.Major_Genre == \"Drama\" # column object of TYPE boolean\n",
    "good_rating_filter = movies_df.IMDB_Rating > 7.0\n",
    "# can use & (and), | (or), ~ (not)\n",
    "good_drama_filter = good_rating_filter & drama_filter\n",
    "\n",
    "# can use boolean column objects as arguments to filter\n",
    "good_dramas_df = movies_df.filter(good_drama_filter).select(\"Title\", \"Major_Genre\", \"IMDB_Rating\")\n",
    "good_dramas_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7be5686-e421-4160-bc92-0bf81437ff1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+--------------+\n",
      "|Title                     |IsItAGoodDrama|\n",
      "+--------------------------+--------------+\n",
      "|The Land Girls            |false         |\n",
      "|First Love, Last Rites    |false         |\n",
      "|I Married a Strange Person|false         |\n",
      "|Let's Talk About Sex      |false         |\n",
      "|Slam                      |false         |\n",
      "+--------------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# can add the col object as a column/property for every row\n",
    "movies_with_good_drama_condition_df = movies_df\\\n",
    "    .select(col(\"Title\"), good_drama_filter.alias(\"IsItAGoodDrama\"))\n",
    "\n",
    "movies_with_good_drama_condition_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b11977f-985b-427f-8ac7-82ff8a1084fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+\n",
      "|Title           |IsItAGoodDrama|\n",
      "+----------------+--------------+\n",
      "|12 Angry Men    |true          |\n",
      "|Twelve Monkeys  |true          |\n",
      "|Twin Falls Idaho|true          |\n",
      "|Amen            |true          |\n",
      "|Barry Lyndon    |true          |\n",
      "+----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# can filter using the true/false value of a column\n",
    "good_dramas_df_v2 = movies_with_good_drama_condition_df.filter(\"IsItAGoodDrama\")\n",
    "\n",
    "good_dramas_df_v2.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e655c10-0fd6-4eb0-9302-e4e5d7b3e4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-----------------------------------------------------+\n",
      "|Title                     |(NOT ((IMDB_Rating > 7.0) AND (Major_Genre = Drama)))|\n",
      "+--------------------------+-----------------------------------------------------+\n",
      "|The Land Girls            |true                                                 |\n",
      "|First Love, Last Rites    |true                                                 |\n",
      "|I Married a Strange Person|true                                                 |\n",
      "|Let's Talk About Sex      |true                                                 |\n",
      "|Slam                      |true                                                 |\n",
      "+--------------------------+-----------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# negation\n",
    "bad_drama_filter = ~good_drama_filter\n",
    "bad_dramas = movies_df.select(col(\"Title\"), bad_drama_filter)\n",
    "bad_dramas.show(5, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea652f9-6184-493e-b9bc-29b031b24c46",
   "metadata": {},
   "source": [
    "# Stat num functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7931d7bd-e44a-4ea1-9bd8-2414de9a14fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4259708986248316\n"
     ]
    }
   ],
   "source": [
    "movies_avg_ratings_df = movies_df\\\n",
    "    .select(\n",
    "    col(\"Title\"),\n",
    "    (col(\"Rotten_Tomatoes_Rating\") / 10 + col(\"IMDB_Rating\")) / 2\n",
    ")\n",
    "# can use ==, >=, >, <, <= to obtain boolean col objects\n",
    "\n",
    "\n",
    "# Pearson correlation - for numerical fields\n",
    "# a number [-1, 1]\n",
    "# is an \"action\" (the DF must be evaluated)\n",
    "rating_correlation = movies_df.stat.corr(\"IMDB_Rating\", \"Rotten_Tomatoes_Rating\")\n",
    "print(rating_correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68d0b53-4dc5-4dfa-8a45-3a85dbb42e3f",
   "metadata": {},
   "source": [
    "# String functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "227d9816-a008-438c-ac28-f15211e3666f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+--------------------+-----------+----------+-----------+-----------+-----------------+------------+----------------------+----------------+-------------------------+-----------+------------+--------+---------------+\n",
      "|Creative_Type     |Director      |Distributor         |IMDB_Rating|IMDB_Votes|MPAA_Rating|Major_Genre|Production_Budget|Release_Date|Rotten_Tomatoes_Rating|Running_Time_min|Source                   |Title      |US_DVD_Sales|US_Gross|Worldwide_Gross|\n",
      "+------------------+--------------+--------------------+-----------+----------+-----------+-----------+-----------------+------------+----------------------+----------------+-------------------------+-----------+------------+--------+---------------+\n",
      "|Science Fiction   |Matt Reeves   |Paramount Pictures  |7.4        |136068    |PG-13      |Action     |25000000         |18-Jan-08   |76                    |null            |Original Screenplay      |Cloverfield|29180398    |80048433|170764033      |\n",
      "|Historical Fiction|Jonathan Demme|Walt Disney Pictures|5.3        |102       |R          |Drama      |53000000         |16-Oct-98   |77                    |172             |Based on Book/Short Story|Beloved    |null        |22852487|22852487       |\n",
      "+------------------+--------------+--------------------+-----------+----------+-----------+-----------+-----------------+------------+----------------------+----------------+-------------------------+-----------+------------+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_df.select(initcap(col(\"Title\"))) \n",
    "# capitalize initials of every word in the string\n",
    "# upper(...), lower(...) to uppercase/lowercase\n",
    "\n",
    "movies_df.filter(col(\"Title\").contains(\"love\")).show(5, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d127f743-1117-48ee-acdf-b4dcb8a332e6",
   "metadata": {},
   "source": [
    "# Regexes filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "818f3667-060d-4d07-9724-8f91cea93d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------+\n",
      "|Name                        |regex_extract|\n",
      "+----------------------------+-------------+\n",
      "|volkswagen 1131 deluxe sedan|volkswagen   |\n",
      "|volkswagen super beetle 117 |volkswagen   |\n",
      "|volkswagen model 111        |volkswagen   |\n",
      "|volkswagen type 3           |volkswagen   |\n",
      "|volkswagen 411 (sw)         |volkswagen   |\n",
      "+----------------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cars_df = spark.read.json(\"data/cars\")\n",
    "\n",
    "regexString = \"volkswagen|vw\"\n",
    "vw_df = cars_df.select(\n",
    "    col(\"Name\"),\n",
    "    regexp_extract(col(\"Name\"), regexString, 0).alias(\"regex_extract\")\n",
    ").filter(col(\"regex_extract\") != \"\")\n",
    "\n",
    "vw_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8aa4334d-7d1a-4c81-a2b6-e4b65e390733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------+\n",
      "|Name                        |replacement                 |\n",
      "+----------------------------+----------------------------+\n",
      "|volkswagen 1131 deluxe sedan|Volkswagen 1131 deluxe sedan|\n",
      "|volkswagen super beetle 117 |Volkswagen super beetle 117 |\n",
      "|volkswagen model 111        |Volkswagen model 111        |\n",
      "|volkswagen type 3           |Volkswagen type 3           |\n",
      "|volkswagen 411 (sw)         |Volkswagen 411 (sw)         |\n",
      "+----------------------------+----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vw_new_name_df = vw_df.select(\n",
    "    col(\"Name\"),\n",
    "    regexp_replace(col(\"Name\"), regexString, \"Volkswagen\").alias(\"replacement\")\n",
    ")\n",
    "vw_new_name_df.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad28e0a9-eef3-4bb5-81a6-a1a85e4ceb21",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "    Filter the cars DF, return all cars whose name contains either element of the list\n",
    "    - contains function\n",
    "    - regexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38099f66-174a-4e9a-ab38-e006d6d1a4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|Name|regex_extract|\n",
      "+----+-------------+\n",
      "+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_car_names():\n",
    "    return [\"Volkswagen\", \"Mercedes-Benz\", \"Ford\"]\n",
    "\n",
    "# v1 - regexes\n",
    "regexString = \"|\".join(get_car_names()) # Volkswagen|Mercedes-Benz|Ford\n",
    "cars_interest_df = cars_df.select(\n",
    "        col(\"Name\"),\n",
    "        regexp_extract(lower(col(\"Name\")), regexString, 0).alias(\"regex_extract\")\n",
    "    ).filter(col(\"regex_extract\") != \"\").orderBy(col(\"regex_extract\"))\n",
    "\n",
    "cars_interest_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5aeae3f9-ac87-42cc-a684-93813ba33661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+------------+----------+----------------+---------------------+------+-------------+----------+\n",
      "|Acceleration|Cylinders|Displacement|Horsepower|Miles_per_Gallon|Name                 |Origin|Weight_in_lbs|Year      |\n",
      "+------------+---------+------------+----------+----------------+---------------------+------+-------------+----------+\n",
      "|10.5        |8        |302.0       |140       |17.0            |ford torino          |USA   |3449         |1970-01-01|\n",
      "|10.0        |8        |429.0       |198       |15.0            |ford galaxie 500     |USA   |4341         |1970-01-01|\n",
      "|11.0        |8        |351.0       |153       |null            |ford torino (sw)     |USA   |4034         |1970-01-01|\n",
      "|8.0         |8        |302.0       |140       |null            |ford mustang boss 302|USA   |3353         |1970-01-01|\n",
      "|16.0        |6        |200.0       |85        |21.0            |ford maverick        |USA   |2587         |1970-01-01|\n",
      "+------------+---------+------------+----------+----------------+---------------------+------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# v2 - contains\n",
    "from functools import reduce\n",
    "\n",
    "car_name_filters = [col(\"Name\").contains(car_name.lower()) for car_name in [\"Volkswagen\", \"Mercedes-Benz\", \"Ford\"]]\n",
    "big_filter = reduce(lambda filter1, filter2: filter1 | filter2, car_name_filters)\n",
    "filtered_cars = cars_df.filter(big_filter)\n",
    "\n",
    "filtered_cars.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f23bd46-a7f5-4c2d-92e5-fb70ca721373",
   "metadata": {},
   "source": [
    "# Date type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5ae2f51-eb6f-4d0f-9624-0d56d917bfd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o284.showString.\n: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'dd-MMM-YY' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToRecognizePatternAfterUpgradeError(QueryExecutionErrors.scala:936)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:187)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:180)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:153)\n\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.getFormatter(TimestampFormatter.scala:351)\n\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.apply(TimestampFormatter.scala:394)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.getFormatter(datetimeExpressions.scala:90)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.getFormatter$(datetimeExpressions.scala:84)\n\tat org.apache.spark.sql.catalyst.expressions.ToTimestamp.getFormatter(datetimeExpressions.scala:1169)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.$anonfun$formatterOption$1(datetimeExpressions.scala:81)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.formatterOption(datetimeExpressions.scala:81)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.formatterOption$(datetimeExpressions.scala:79)\n\tat org.apache.spark.sql.catalyst.expressions.ToTimestamp.formatterOption$lzycompute(datetimeExpressions.scala:1169)\n\tat org.apache.spark.sql.catalyst.expressions.ToTimestamp.formatterOption(datetimeExpressions.scala:1169)\n\tat org.apache.spark.sql.catalyst.expressions.ToTimestamp.doGenCode(datetimeExpressions.scala:1246)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:151)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:146)\n\tat org.apache.spark.sql.catalyst.expressions.CastBase.doGenCode(Cast.scala:936)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:151)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:146)\n\tat org.apache.spark.sql.catalyst.expressions.CastBase.genCode(Cast.scala:931)\n\tat org.apache.spark.sql.catalyst.expressions.CastBase.doGenCode(Cast.scala:936)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:151)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:146)\n\tat org.apache.spark.sql.catalyst.expressions.CastBase.genCode(Cast.scala:931)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.genCode(namedExpressions.scala:171)\n\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doConsume$2(basicPhysicalOperators.scala:73)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doConsume$1(basicPhysicalOperators.scala:73)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.withSubExprEliminationExprs(CodeGenerator.scala:1039)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:73)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:195)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:150)\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:497)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:484)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:457)\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:497)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:96)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:222)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:219)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:91)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:91)\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:497)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:96)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:222)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:219)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:91)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:91)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:659)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:722)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:184)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:222)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:219)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:325)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:443)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\n\tat jdk.internal.reflect.GeneratedMethodAccessor54.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.IllegalArgumentException: All week-based patterns are unsupported since Spark 3.0, detected: Y, Please use the SQL function EXTRACT instead\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4(DateTimeFormatterHelper.scala:319)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4$adapted(DateTimeFormatterHelper.scala:317)\n\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:985)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.immutable.StringOps.foreach(StringOps.scala:33)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:984)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$2(DateTimeFormatterHelper.scala:317)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.convertIncompatiblePattern(DateTimeFormatterHelper.scala:314)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter(DateTimeFormatterHelper.scala:121)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter$(DateTimeFormatterHelper.scala:117)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.getOrCreateFormatter(TimestampFormatter.scala:92)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter$lzycompute(TimestampFormatter.scala:101)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter(TimestampFormatter.scala:100)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:152)\n\t... 93 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m movies_with_release_dates_df \u001b[38;5;241m=\u001b[39m movies_df\u001b[38;5;241m.\u001b[39mselect(\n\u001b[1;32m      2\u001b[0m     col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      3\u001b[0m     to_date(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelease_Date\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdd-MMM-YY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#     to_date(col(\"Release_Date\"), \"dd-MMM-YY\").alias(\"Actual_Release\")\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmovies_with_release_dates_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:494\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o284.showString.\n: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'dd-MMM-YY' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failToRecognizePatternAfterUpgradeError(QueryExecutionErrors.scala:936)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:187)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:180)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:153)\n\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.getFormatter(TimestampFormatter.scala:351)\n\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.apply(TimestampFormatter.scala:394)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.getFormatter(datetimeExpressions.scala:90)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.getFormatter$(datetimeExpressions.scala:84)\n\tat org.apache.spark.sql.catalyst.expressions.ToTimestamp.getFormatter(datetimeExpressions.scala:1169)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.$anonfun$formatterOption$1(datetimeExpressions.scala:81)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.formatterOption(datetimeExpressions.scala:81)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.formatterOption$(datetimeExpressions.scala:79)\n\tat org.apache.spark.sql.catalyst.expressions.ToTimestamp.formatterOption$lzycompute(datetimeExpressions.scala:1169)\n\tat org.apache.spark.sql.catalyst.expressions.ToTimestamp.formatterOption(datetimeExpressions.scala:1169)\n\tat org.apache.spark.sql.catalyst.expressions.ToTimestamp.doGenCode(datetimeExpressions.scala:1246)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:151)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:146)\n\tat org.apache.spark.sql.catalyst.expressions.CastBase.doGenCode(Cast.scala:936)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:151)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:146)\n\tat org.apache.spark.sql.catalyst.expressions.CastBase.genCode(Cast.scala:931)\n\tat org.apache.spark.sql.catalyst.expressions.CastBase.doGenCode(Cast.scala:936)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:151)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:146)\n\tat org.apache.spark.sql.catalyst.expressions.CastBase.genCode(Cast.scala:931)\n\tat org.apache.spark.sql.catalyst.expressions.Alias.genCode(namedExpressions.scala:171)\n\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doConsume$2(basicPhysicalOperators.scala:73)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.ProjectExec.$anonfun$doConsume$1(basicPhysicalOperators.scala:73)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.withSubExprEliminationExprs(CodeGenerator.scala:1039)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:73)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:195)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:150)\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:497)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:484)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:457)\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:497)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:96)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:222)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:219)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:91)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:91)\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:497)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:96)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:222)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:219)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:91)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:91)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:659)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:722)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:184)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:222)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:219)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:325)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:443)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\n\tat jdk.internal.reflect.GeneratedMethodAccessor54.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.IllegalArgumentException: All week-based patterns are unsupported since Spark 3.0, detected: Y, Please use the SQL function EXTRACT instead\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4(DateTimeFormatterHelper.scala:319)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4$adapted(DateTimeFormatterHelper.scala:317)\n\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:985)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.immutable.StringOps.foreach(StringOps.scala:33)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:984)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$2(DateTimeFormatterHelper.scala:317)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.convertIncompatiblePattern(DateTimeFormatterHelper.scala:314)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter(DateTimeFormatterHelper.scala:121)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter$(DateTimeFormatterHelper.scala:117)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.getOrCreateFormatter(TimestampFormatter.scala:92)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter$lzycompute(TimestampFormatter.scala:101)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter(TimestampFormatter.scala:100)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:152)\n\t... 93 more\n"
     ]
    }
   ],
   "source": [
    "# TODO Not working in Spark 2.4\n",
    "# How to conver data\n",
    "\n",
    "movies_with_release_dates_df = movies_df.select(\n",
    "    col(\"Title\"),\n",
    "    to_date(col(\"Release_Date\"), \"dd-MMM-YY\")\n",
    ")\n",
    "\n",
    "#     to_date(col(\"Release_Date\"), \"dd-MMM-YY\").alias(\"Actual_Release\")\n",
    "\n",
    "movies_with_release_dates_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "796a105f-9837-4b15-9ddd-25d04232e981",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve 'Actual_Release' given input columns: [Right_Now, Title, Today, to_date(Release_Date, dd-MMM-YY)];\n'Project [Title#19, to_date(Release_Date, dd-MMM-YY)#618, Today#630, Right_Now#634, (datediff(Today#630, 'Actual_Release) / 365) AS Movie_Age#639]\n+- Project [Title#19, to_date(Release_Date, dd-MMM-YY)#618, Today#630, current_timestamp() AS Right_Now#634]\n   +- Project [Title#19, to_date(Release_Date, dd-MMM-YY)#618, current_date(Some(Etc/UTC)) AS Today#630]\n      +- Project [Title#19, to_date('Release_Date, Some(dd-MMM-YY)) AS to_date(Release_Date, dd-MMM-YY)#618]\n         +- Relation [Creative_Type#7,Director#8,Distributor#9,IMDB_Rating#10,IMDB_Votes#11L,MPAA_Rating#12,Major_Genre#13,Production_Budget#14L,Release_Date#15,Rotten_Tomatoes_Rating#16L,Running_Time_min#17L,Source#18,Title#19,US_DVD_Sales#20L,US_Gross#21L,Worldwide_Gross#22L] json\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# date operations\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m enriched_movies_df \u001b[38;5;241m=\u001b[39m \u001b[43mmovies_with_release_dates_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mToday\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_date\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRight_Now\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_timestamp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMovie_Age\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatediff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mToday\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mActual_Release\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m365\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m enriched_movies_df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:2478\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   2476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql_ctx)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'Actual_Release' given input columns: [Right_Now, Title, Today, to_date(Release_Date, dd-MMM-YY)];\n'Project [Title#19, to_date(Release_Date, dd-MMM-YY)#618, Today#630, Right_Now#634, (datediff(Today#630, 'Actual_Release) / 365) AS Movie_Age#639]\n+- Project [Title#19, to_date(Release_Date, dd-MMM-YY)#618, Today#630, current_timestamp() AS Right_Now#634]\n   +- Project [Title#19, to_date(Release_Date, dd-MMM-YY)#618, current_date(Some(Etc/UTC)) AS Today#630]\n      +- Project [Title#19, to_date('Release_Date, Some(dd-MMM-YY)) AS to_date(Release_Date, dd-MMM-YY)#618]\n         +- Relation [Creative_Type#7,Director#8,Distributor#9,IMDB_Rating#10,IMDB_Votes#11L,MPAA_Rating#12,Major_Genre#13,Production_Budget#14L,Release_Date#15,Rotten_Tomatoes_Rating#16L,Running_Time_min#17L,Source#18,Title#19,US_DVD_Sales#20L,US_Gross#21L,Worldwide_Gross#22L] json\n"
     ]
    }
   ],
   "source": [
    "# date operations\n",
    "enriched_movies_df = movies_with_release_dates_df. \\\n",
    "    withColumn(\"Today\", current_date()). \\\n",
    "    withColumn(\"Right_Now\", current_timestamp()). \\\n",
    "    withColumn(\"Movie_Age\", datediff(col(\"Today\"), col(\"Actual_Release\")) / 365)\n",
    "\n",
    "\n",
    "enriched_movies_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63232adf-8abb-4397-8b10-3bf346cd40e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve 'Actual_Release' given input columns: [Title, to_date(Release_Date, dd-MMM-YY)];\n'Filter isnull('Actual_Release)\n+- Project [Title#19, to_date('Release_Date, Some(dd-MMM-YY)) AS to_date(Release_Date, dd-MMM-YY)#618]\n   +- Relation [Creative_Type#7,Director#8,Distributor#9,IMDB_Rating#10,IMDB_Votes#11L,MPAA_Rating#12,Major_Genre#13,Production_Budget#14L,Release_Date#15,Rotten_Tomatoes_Rating#16L,Running_Time_min#17L,Source#18,Title#19,US_DVD_Sales#20L,US_Gross#21L,Worldwide_Gross#22L] json\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# check for empty date\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m no_release_known_df \u001b[38;5;241m=\u001b[39m \u001b[43mmovies_with_release_dates_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mActual_Release\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misNull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m no_release_known_df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:1733\u001b[0m, in \u001b[0;36mDataFrame.filter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   1731\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mfilter(condition)\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(condition, Column):\n\u001b[0;32m-> 1733\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcondition should be string or Column\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'Actual_Release' given input columns: [Title, to_date(Release_Date, dd-MMM-YY)];\n'Filter isnull('Actual_Release)\n+- Project [Title#19, to_date('Release_Date, Some(dd-MMM-YY)) AS to_date(Release_Date, dd-MMM-YY)#618]\n   +- Relation [Creative_Type#7,Director#8,Distributor#9,IMDB_Rating#10,IMDB_Votes#11L,MPAA_Rating#12,Major_Genre#13,Production_Budget#14L,Release_Date#15,Rotten_Tomatoes_Rating#16L,Running_Time_min#17L,Source#18,Title#19,US_DVD_Sales#20L,US_Gross#21L,Worldwide_Gross#22L] json\n"
     ]
    }
   ],
   "source": [
    "# check for empty date\n",
    "no_release_known_df = movies_with_release_dates_df.filter(col(\"Actual_Release\").isNull())\n",
    "no_release_known_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb69ea2a-6e49-4493-8de6-d84585bfbf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothetical\n",
    "movies_with_2_formats = movies_df.select(col(\"Title\"), col(\"Release_Date\")). \\\n",
    "    withColumn(\"Date_F1\", to_date(col(\"Release_Date\"), \"dd-MM-yyyy\")). \\\n",
    "    withColumn(\"Date_F2\", to_date(col(\"Release_Date\"), \"yyyy-MM-dd\")). \\\n",
    "    withColumn(\"Actual_Date\", coalesce(col(\"Date_F1\"), col(\"Date_F2\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1136b515-7985-4d1e-931c-b89a3178ee90",
   "metadata": {},
   "source": [
    "# Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aaf9ac3f-9089-42fa-b874-2810b3b03c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "structures create\n",
      "+--------------------+--------------------+\n",
      "|               Title|              Profit|\n",
      "+--------------------+--------------------+\n",
      "|      The Land Girls|{146083, 146083, ...|\n",
      "|First Love, Last ...|{10876, 10876, null}|\n",
      "|I Married a Stran...|{203134, 203134, ...|\n",
      "|Let's Talk About Sex|{373615, 373615, ...|\n",
      "|                Slam|{1009819, 1087521...|\n",
      "| Mississippi Mermaid|{24551, 2624551, ...|\n",
      "|           Following|{44705, 44705, null}|\n",
      "|             Foolish|{6026908, 6026908...|\n",
      "|             Pirates|{1641825, 6341825...|\n",
      "|     Duel in the Sun|{20400000, 204000...|\n",
      "|           Tom Jones|{37600000, 376000...|\n",
      "|             Oliver!|{37402877, 374028...|\n",
      "|To Kill A Mocking...|{13129846, 131298...|\n",
      "|    Tora, Tora, Tora|{29548291, 295482...|\n",
      "|   Hollywood Shuffle|{5228617, 5228617...|\n",
      "|Over the Hill to ...|{3000000, 3000000...|\n",
      "|              Wilson|{2000000, 2000000...|\n",
      "|        Darling Lili|{5000000, 5000000...|\n",
      "|The Ten Commandments|{80000000, 800000...|\n",
      "|        12 Angry Men|        {0, 0, null}|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# structures\n",
    "print(\"structures create\")\n",
    "movies_struct_df = movies_df. \\\n",
    "    select(col(\"Title\"), struct(col(\"US_Gross\"), col(\"Worldwide_Gross\"), col(\"US_DVD_Sales\")).alias(\"Profit\"))\n",
    "\n",
    "movies_struct_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf0ddd53-7b0e-4573-b5bb-f6ba72c75193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|               Title|US_Profit|\n",
      "+--------------------+---------+\n",
      "|      The Land Girls|   146083|\n",
      "|First Love, Last ...|    10876|\n",
      "|I Married a Stran...|   203134|\n",
      "|Let's Talk About Sex|   373615|\n",
      "|                Slam|  1009819|\n",
      "| Mississippi Mermaid|    24551|\n",
      "|           Following|    44705|\n",
      "|             Foolish|  6026908|\n",
      "|             Pirates|  1641825|\n",
      "|     Duel in the Sun| 20400000|\n",
      "|           Tom Jones| 37600000|\n",
      "|             Oliver!| 37402877|\n",
      "|To Kill A Mocking...| 13129846|\n",
      "|    Tora, Tora, Tora| 29548291|\n",
      "|   Hollywood Shuffle|  5228617|\n",
      "|Over the Hill to ...|  3000000|\n",
      "|              Wilson|  2000000|\n",
      "|        Darling Lili|  5000000|\n",
      "|The Ten Commandments| 80000000|\n",
      "|        12 Angry Men|        0|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get fields\n",
    "\n",
    "movies_struct_df. \\\n",
    "    select(col(\"Title\"), col(\"Profit\").getField(\"US_Gross\").alias(\"US_Profit\")).\\\n",
    "    show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2704a040-ee97-40c4-b63c-9bd0e4f232b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|               Title|US_Profit|\n",
      "+--------------------+---------+\n",
      "|      The Land Girls|   146083|\n",
      "|First Love, Last ...|    10876|\n",
      "|I Married a Stran...|   203134|\n",
      "|Let's Talk About Sex|   373615|\n",
      "|                Slam|  1009819|\n",
      "| Mississippi Mermaid|    24551|\n",
      "|           Following|    44705|\n",
      "|             Foolish|  6026908|\n",
      "|             Pirates|  1641825|\n",
      "|     Duel in the Sun| 20400000|\n",
      "|           Tom Jones| 37600000|\n",
      "|             Oliver!| 37402877|\n",
      "|To Kill A Mocking...| 13129846|\n",
      "|    Tora, Tora, Tora| 29548291|\n",
      "|   Hollywood Shuffle|  5228617|\n",
      "|Over the Hill to ...|  3000000|\n",
      "|              Wilson|  2000000|\n",
      "|        Darling Lili|  5000000|\n",
      "|The Ten Commandments| 80000000|\n",
      "|        12 Angry Men|        0|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# structures - SQL expression strings\n",
    "movies_struct_df_v2 = movies_df. \\\n",
    "    selectExpr(\"Title\", \"(US_Gross, Worldwide_Gross, US_DVD_Sales) as Profit\"). \\\n",
    "    selectExpr(\"Title\", \"Profit.US_Gross as US_Profit\")\n",
    "\n",
    "movies_struct_df_v2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f67a743-360a-4f88-8639-30f21c0db1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nested data structures\n",
      "+--------------------+--------------------+\n",
      "|               Title|             Success|\n",
      "+--------------------+--------------------+\n",
      "|      The Land Girls|{{6.1, null}, {14...|\n",
      "|First Love, Last ...|{{6.9, null}, {10...|\n",
      "|I Married a Stran...|{{6.8, null}, {20...|\n",
      "|Let's Talk About Sex|{{null, 13}, {373...|\n",
      "|                Slam|{{3.4, 62}, {1009...|\n",
      "| Mississippi Mermaid|{{null, null}, {2...|\n",
      "|           Following|{{7.7, null}, {44...|\n",
      "|             Foolish|{{3.8, null}, {60...|\n",
      "|             Pirates|{{5.8, 25}, {1641...|\n",
      "|     Duel in the Sun|{{7.0, 86}, {2040...|\n",
      "|           Tom Jones|{{7.0, 81}, {3760...|\n",
      "|             Oliver!|{{7.5, 84}, {3740...|\n",
      "|To Kill A Mocking...|{{8.4, 97}, {1312...|\n",
      "|    Tora, Tora, Tora|{{null, null}, {2...|\n",
      "|   Hollywood Shuffle|{{6.8, 87}, {5228...|\n",
      "|Over the Hill to ...|{{null, null}, {3...|\n",
      "|              Wilson|{{7.0, null}, {20...|\n",
      "|        Darling Lili|{{6.1, null}, {50...|\n",
      "|The Ten Commandments|{{2.5, 90}, {8000...|\n",
      "|        12 Angry Men|{{8.9, null}, {0,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# very nested data structures\n",
    "movies_struct_df_v3 = movies_df. \\\n",
    "    selectExpr(\"Title\",\n",
    "               \"((IMDB_Rating, Rotten_Tomatoes_Rating) as Rating, (US_Gross, Worldwide_Gross, US_DVD_Sales) as Profit) as Success\")\n",
    "print(\"nested data structures\")\n",
    "\n",
    "movies_struct_df_v3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ad2c743-9671-4333-bc93-ee64ecf225d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|               Title|IMDB|\n",
      "+--------------------+----+\n",
      "|      The Land Girls| 6.1|\n",
      "|First Love, Last ...| 6.9|\n",
      "|I Married a Stran...| 6.8|\n",
      "|Let's Talk About Sex|null|\n",
      "|                Slam| 3.4|\n",
      "| Mississippi Mermaid|null|\n",
      "|           Following| 7.7|\n",
      "|             Foolish| 3.8|\n",
      "|             Pirates| 5.8|\n",
      "|     Duel in the Sun| 7.0|\n",
      "|           Tom Jones| 7.0|\n",
      "|             Oliver!| 7.5|\n",
      "|To Kill A Mocking...| 8.4|\n",
      "|    Tora, Tora, Tora|null|\n",
      "|   Hollywood Shuffle| 6.8|\n",
      "|Over the Hill to ...|null|\n",
      "|              Wilson| 7.0|\n",
      "|        Darling Lili| 6.1|\n",
      "|The Ten Commandments| 2.5|\n",
      "|        12 Angry Men| 8.9|\n",
      "+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|               Title|             Success|\n",
      "+--------------------+--------------------+\n",
      "|      The Land Girls|{{6.1, null}, {14...|\n",
      "|First Love, Last ...|{{6.9, null}, {10...|\n",
      "|I Married a Stran...|{{6.8, null}, {20...|\n",
      "|Let's Talk About Sex|{{null, 13}, {373...|\n",
      "|                Slam|{{3.4, 62}, {1009...|\n",
      "| Mississippi Mermaid|{{null, null}, {2...|\n",
      "|           Following|{{7.7, null}, {44...|\n",
      "|             Foolish|{{3.8, null}, {60...|\n",
      "|             Pirates|{{5.8, 25}, {1641...|\n",
      "|     Duel in the Sun|{{7.0, 86}, {2040...|\n",
      "|           Tom Jones|{{7.0, 81}, {3760...|\n",
      "|             Oliver!|{{7.5, 84}, {3740...|\n",
      "|To Kill A Mocking...|{{8.4, 97}, {1312...|\n",
      "|    Tora, Tora, Tora|{{null, null}, {2...|\n",
      "|   Hollywood Shuffle|{{6.8, 87}, {5228...|\n",
      "|Over the Hill to ...|{{null, null}, {3...|\n",
      "|              Wilson|{{7.0, null}, {20...|\n",
      "|        Darling Lili|{{6.1, null}, {50...|\n",
      "|The Ten Commandments|{{2.5, 90}, {8000...|\n",
      "|        12 Angry Men|{{8.9, null}, {0,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_struct_df_v3. \\\n",
    "    selectExpr(\"Title\", \"Success.Rating.IMDB_Rating as IMDB\").show()\n",
    "\n",
    "movies_struct_df_v3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41f7ab3-5a76-4196-ad2c-9f548ac3e0c0",
   "metadata": {},
   "source": [
    "# arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "574a6468-e765-492d-a269-6c5061b535fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Title_Words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- Director_Words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+\n",
      "|               Title|         Title_Words|      Director_Words|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|      The Land Girls|  [The, Land, Girls]|                null|\n",
      "|First Love, Last ...|[First, Love, , L...|                null|\n",
      "|I Married a Stran...|[I, Married, a, S...|                null|\n",
      "|Let's Talk About Sex|[Let's, Talk, Abo...|                null|\n",
      "|                Slam|              [Slam]|                null|\n",
      "| Mississippi Mermaid|[Mississippi, Mer...|                null|\n",
      "|           Following|         [Following]|[Christopher, Nolan]|\n",
      "|             Foolish|           [Foolish]|                null|\n",
      "|             Pirates|           [Pirates]|   [Roman, Polanski]|\n",
      "|     Duel in the Sun|[Duel, in, the, Sun]|                null|\n",
      "|           Tom Jones|        [Tom, Jones]|                null|\n",
      "|             Oliver!|           [Oliver!]|                null|\n",
      "|To Kill A Mocking...|[To, Kill, A, Moc...|                null|\n",
      "|    Tora, Tora, Tora|[Tora, , Tora, , ...|[Richard, Fleischer]|\n",
      "|   Hollywood Shuffle|[Hollywood, Shuffle]|                null|\n",
      "|Over the Hill to ...|[Over, the, Hill,...|                null|\n",
      "|              Wilson|            [Wilson]|                null|\n",
      "|        Darling Lili|     [Darling, Lili]|    [Blake, Edwards]|\n",
      "|The Ten Commandments|[The, Ten, Comman...|                null|\n",
      "|        12 Angry Men|    [12, Angry, Men]|     [Sidney, Lumet]|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "movies_with_words_df = movies_df.\\\n",
    "    select(col(\"Title\"),\n",
    "    split(col(\"Title\"), \" |,\").alias(\"Title_Words\"),\n",
    "    split(col(\"Director\"), \" |,\").alias(\"Director_Words\"))\n",
    "\n",
    "movies_with_words_df.printSchema()\n",
    "movies_with_words_df.show()\n",
    "\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^ col object of type ARRAY[String]\n",
    "# you can have nested arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2fd689ea-b476-494c-915e-6a5354d5e6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+-----------------+---------------------------------+\n",
      "|               Title|Title_Words[0]|size(Title_Words)|array_contains(Title_Words, Love)|\n",
      "+--------------------+--------------+-----------------+---------------------------------+\n",
      "|      The Land Girls|           The|                3|                            false|\n",
      "|First Love, Last ...|         First|                5|                             true|\n",
      "|I Married a Stran...|             I|                5|                            false|\n",
      "|Let's Talk About Sex|         Let's|                4|                            false|\n",
      "|                Slam|          Slam|                1|                            false|\n",
      "| Mississippi Mermaid|   Mississippi|                2|                            false|\n",
      "|           Following|     Following|                1|                            false|\n",
      "|             Foolish|       Foolish|                1|                            false|\n",
      "|             Pirates|       Pirates|                1|                            false|\n",
      "|     Duel in the Sun|          Duel|                4|                            false|\n",
      "|           Tom Jones|           Tom|                2|                            false|\n",
      "|             Oliver!|       Oliver!|                1|                            false|\n",
      "|To Kill A Mocking...|            To|                4|                            false|\n",
      "|    Tora, Tora, Tora|          Tora|                5|                            false|\n",
      "|   Hollywood Shuffle|     Hollywood|                2|                            false|\n",
      "|Over the Hill to ...|          Over|                6|                            false|\n",
      "|              Wilson|        Wilson|                1|                            false|\n",
      "|        Darling Lili|       Darling|                2|                            false|\n",
      "|The Ten Commandments|           The|                3|                            false|\n",
      "|        12 Angry Men|            12|                3|                            false|\n",
      "+--------------------+--------------+-----------------+---------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# array operations\n",
    "array_ops_df = movies_with_words_df.select(\n",
    "    col(\"Title\"),\n",
    "    expr(\"Title_Words[0]\"),  # the first element in the array\n",
    "    size(col(\"Title_Words\")),  # the length of the array\n",
    "    array_contains(col(\"Title_Words\"), \"Love\")\n",
    "    # a bunch of array_(...) functions\n",
    ")\n",
    "\n",
    "array_ops_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5930d965-c3d8-42dd-9f5b-f860bf377131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|               Title|        col|\n",
      "+--------------------+-----------+\n",
      "|      The Land Girls|        The|\n",
      "|      The Land Girls|       Land|\n",
      "|      The Land Girls|      Girls|\n",
      "|First Love, Last ...|      First|\n",
      "|First Love, Last ...|       Love|\n",
      "|First Love, Last ...|           |\n",
      "|First Love, Last ...|       Last|\n",
      "|First Love, Last ...|      Rites|\n",
      "|I Married a Stran...|          I|\n",
      "|I Married a Stran...|    Married|\n",
      "|I Married a Stran...|          a|\n",
      "|I Married a Stran...|    Strange|\n",
      "|I Married a Stran...|     Person|\n",
      "|Let's Talk About Sex|      Let's|\n",
      "|Let's Talk About Sex|       Talk|\n",
      "|Let's Talk About Sex|      About|\n",
      "|Let's Talk About Sex|        Sex|\n",
      "|                Slam|       Slam|\n",
      "| Mississippi Mermaid|Mississippi|\n",
      "| Mississippi Mermaid|    Mermaid|\n",
      "+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Flat arrays\n",
    "\n",
    "array_ops_df = movies_with_words_df.select(\n",
    "    col(\"Title\"),\n",
    "    explode(col(\"Title_Words\"))\n",
    ")\n",
    "\n",
    "array_ops_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f07b78c-bc32-4bfb-a5dc-c0cb9c94a4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
